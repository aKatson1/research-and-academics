{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8092223b-b2d1-4b73-9d36-1c42e0b0e92f",
   "metadata": {},
   "source": [
    "#  PCA Portfolio Variance Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbff24c-8c01-471f-9820-2ef0bb016b59",
   "metadata": {},
   "source": [
    "This notebook demonstrates SVD-based Principal Component Analysis (PCA) for portfolio risk analysis and illustrates its relationship to the traditional covariance-matrix formulation of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d3e1e-eb4e-433a-9f3d-2ff64ff73c51",
   "metadata": {},
   "source": [
    "## SVD Algorithm \n",
    "\n",
    "Recall that SVD decomposes a matrix $A \\in \\mathbb{R}^{m \\times n}$ into the form $A=U \\Sigma V^T$, which is computed in the following way: \n",
    "- Determine the eigenvalues $\\lambda_i$ and eigenvectors $v_i$ of $A^TA$, which always has real, nonnegative eigenvalues since it is square, symmetric, and positive semidefinite. Order them descending $\\lambda_1\\geq\\lambda_2\\geq...\\geq0$ with coresponding ordered eigenvectors $v_i$. \n",
    "- Construct $m \\times n$ diagonal matrix $\\Sigma = diag(\\sigma_1, \\sigma_2,...)$ where $\\sigma_i = \\sqrt{\\lambda_i}$ are the singular values.\n",
    "- Construct  $n \\times n$ orthogonal matrix $V= [v_1, v_2,...,v_n]$ with orthonormal eigenvectors $v_i$.\n",
    "- Construct $m \\times m$ orthogonal matrix $U$ by the formula $u_i = \\frac{Av_i}{\\sigma_i}$ for $\\sigma_i \\neq 0$. \n",
    "\n",
    "This means that $A^TAV = (V \\Sigma^TU^T)(U \\Sigma V^T)V = V(\\Sigma^T \\Sigma)\\Longleftrightarrow A^TA v_i = \\sigma_i^2 v_i$, i.e. $V$ contains the eigenvectors of $A^TA$, and $\\Sigma$ contains the roots of the eigenvalues. Likewise, $AA^T u_i = \\sigma_i^2 u_i$, so $U$ contains the eigenvectors of $AA^T$. \n",
    "Intuitively, the computation $A\\vec{x}= U\\Sigma V^T\\vec{x}$, any generic linear transformation that stretches the unit sphere in $\\mathbb{R}^n$  into a scaled ellipsoid in $\\mathbb{R}^m$, is equivalent to \n",
    "- Rotation into principal directions ($V^T$) \n",
    "- A stretch or compression along the principal axes by corresponding singular values ($\\Sigma$)\n",
    "- Rotation back into the output space ($U$)\n",
    "\n",
    "Note: NumPy may store diagonal matrices as 1D arrays of singular values for memory efficiency, as you may see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6da177be-2e41-4b10-b60c-161c068ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:  [[-0.21483724  0.88723069  0.40824829]\n",
      " [-0.52058739  0.24964395 -0.81649658]\n",
      " [-0.82633754 -0.38794278  0.40824829]] \n",
      "Sigma:  [1.68481034e+01 1.06836951e+00 3.33475287e-16] \n",
      "V^T:  [[-0.47967118 -0.57236779 -0.66506441]\n",
      " [-0.77669099 -0.07568647  0.62531805]\n",
      " [-0.40824829  0.81649658 -0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]]) \n",
    "U, S, VT = np.linalg.svd(A) \n",
    "\n",
    "print(\"U: \", U, \"\\nSigma: \", S, \"\\nV^T: \", VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90141b23-740a-40c0-9888-10943ca56f50",
   "metadata": {},
   "source": [
    "## Principal Directions, Lagrange Derivation, and Low-Rank Approximation\n",
    "\n",
    "Rigorously, the SVD determines the direction $\\vec{v}$ with $||\\vec{v}||=1$ that maximizes the quantity $||A\\vec{v}||$. One can prove this with  Lagrange:\n",
    "- Find max $f(\\vec{v}) = ||A\\vec{v}||^2=\\vec{v}^T(A^TA)\\vec{v}$ subject to constraint $g(\\vec{v})=||\\vec{v}||^2=\\vec{v}^T\\vec{v}=v_1^2+v_2^2+...=1$. One can view $f(\\vec{v})$ as a surface over a unit sphere $g(\\vec{v})=1$ in $\\mathbb{R}^n$.\n",
    "- Then we solve for $\\nabla f(\\vec{v}) || \\nabla g(\\vec{v}) \\Longleftrightarrow \\nabla f(\\vec{v})= \\lambda \\nabla g(\\vec{v}) $. Note that $\\nabla g(\\vec{v})= 2\\vec{v}$ and $\\nabla f( \\vec{v} ) = 2A^TA\\vec{v}$ implies $A^TA\\vec{v} = \\lambda \\vec{v}$\n",
    "- This is exactly the problem of solving for eigenvectors and eigenvalues of $A^TA$. Moreover, $||A\\vec{v}||^2= \\vec{v}^TA^TA\\vec{v}= \\lambda ||\\vec{v}||^2$ takes max value for the largest eigenvalue.\n",
    "\n",
    "Moreover, since the eigenvectors of different eigenvalues of a symmetric matrix are orthogonal, we maximize $||A\\vec{v}|$ in perpendicular directions. Thus, PCA identifies the directions of maximal $||A\\vec{v}||$, ordered by value of $\\lambda$, in perpendicular directions. This is why it is useful for covariance matrices-- we can see which direction results in greatest variance.\n",
    "\n",
    "It has been shown that the best lower rank (i.e. lower dimensional) approximation of a matrix $A$ as measured by the Frobenius norm $||A||_F = \\sqrt{\\sum_i\\sum_j|a_{ij}|^2}$ is the SVD. The Forbenius error is given by $||A-A_k||_F^2=\\sum_{i=k+1}^{r}\\sigma_i^2$ and the amount of variance accounted for by our approximation in $k$ of our $r$ principal directions is given by $\\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2}$. \n",
    "\n",
    "Below is an example showing how to retain the two largest singular values of a $3 \\times 3$ matrix and compute the approximation error using the Frobenius norm (default in NumPy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "961487dc-bf4d-4675-927b-cf726bb12e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (5, 2) Sigma shape: (2, 2) V shape: (2, 3) A shape: (5, 3)\n",
      "Singular Values: [5.94177114 0.22267995 0.07595627]\n",
      "Approximation error using 2 components: 0.07595627220785459  (≈ third singular value).\n",
      "The first 2 principal components account for 99.984% of total variance.\n",
      "\n",
      "Original Matrix A:\n",
      " [[3.  1.  2. ]\n",
      " [2.5 0.9 1.8]\n",
      " [2.  0.8 1.6]\n",
      " [1.5 0.4 1. ]\n",
      " [0.5 0.2 0.4]]\n",
      "\n",
      "Reconstructed from top 2 PCs:\n",
      " [[2.99573809 0.97021265 2.0206212 ]\n",
      " [2.49889429 0.89227194 1.80534998]\n",
      " [2.00205048 0.81433123 1.59007877]\n",
      " [1.50740932 0.45178529 0.96415006]\n",
      " [0.50051262 0.20358281 0.39751969]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array(\n",
    "[[3.0, 1.0, 2.0],\n",
    " [2.5, 0.9, 1.8],\n",
    " [2.0, 0.8, 1.6],\n",
    " [1.5, 0.4, 1.0],\n",
    " [0.5, 0.2, 0.4]]\n",
    ")\n",
    "\n",
    "\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "k = 2\n",
    "Sigma_k = np.diag(S[:k]) #restrict to k singular values\n",
    "U_k =  U[:,:k] \n",
    "VT_k = VT[:k, :]\n",
    "A_approx =  U_k @ Sigma_k @ VT_k\n",
    "fro_error = np.linalg.norm(A - A_approx, 'fro')\n",
    "variance_acc = 100 * np.sum(S[:k]**2) / np.sum(S**2)\n",
    "\n",
    "print(\"U shape:\", U_k.shape, \"Sigma shape:\", Sigma_k.shape, \"V shape:\", VT_k.shape, \"A shape:\", A.shape)\n",
    "print(\"Singular Values:\", S)\n",
    "print(\"Approximation error using 2 components:\", fro_error,\n",
    "      \" (≈ third singular value).\")\n",
    "print(f\"The first 2 principal components account for {variance_acc:.3f}% of total variance.\")\n",
    "print(\"\\nOriginal Matrix A:\\n\", A)\n",
    "print(\"\\nReconstructed from top 2 PCs:\\n\", A_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5c2e7-2d39-4866-bd7b-c4204c2182af",
   "metadata": {},
   "source": [
    "## SVD, Covariance, and PCA\n",
    "Earlier we saw how SVD is equivalent to solving for $\\vec{v}$ maximizing $$||A\\vec{v}||^2=\\vec{v}^T(A^TA)\\vec{v} \\quad \\text{ subject to  } \\quad ||\\vec{v}||=1$$We now relate it to covariance. Suppose $X \\in \\mathbb{R}^{m \\times n}$ is a *centered* data matrix with rows as variables and columns as observations, then the covariance matrix is given by $\\Sigma=\\frac{1}{n}X^TX$. Then  $$||X\\vec{v}||^2=\\vec{v}^T(X^TX)\\vec{v}=n\\vec{v}^T\\Sigma\\vec{v}.$$ Thus finding $\\vec{v}$ maximizing $||X\\vec{v}||$ is equivalent to finding $\\vec{v}$ maximizing $\\vec{v}^T\\Sigma\\vec{v}$, the direction where projecting onto $\\vec{v}$ yields max variance. \n",
    "\n",
    "The vectors in order in the columns of $U$ give the principal directions when $X$ has rows = features, columns = observations. If the rows = observations and columns = features, then use the rows of $V^T$. The principal directions take the form $PC = w_1 \\text{ stock1} + w_2 \\text{ stock2} + w_3 \\text{ stock3} + w_4 \\text{ stock4} + w_5 \\text{ stock5}$, as linear combination of the original features. Projecting (by calculating $X_{proj}=U^TX$; we use $XV^T$ if rows = observations, columns = features) the 5D stock data onto these PCs gives scalar outputs with the greatest spread of projected values along the line — the direction of maximum historical portfolio variance. If we interpret the PCs as profit functions, they represent the riskiest weighting of stocks: the one whose profit varies the most.\n",
    "\n",
    "![See illustrative GIF in `data/PCAVisual.gif`](../data/PCAVisual.gif)\n",
    "\n",
    "Intuitively: \n",
    "- When all PC1 (first principal component) weights share the same sign, total variance is largest when all stocks rise or fall together along the line/profit function.\n",
    "- If PC1 weights alternate in sign, movements of stocks together partially cancel some of the variance (i.e., do not move directly along the line of max variance), reducing total variance. This is a mathematical expression of diversification. \n",
    "  \n",
    "Important notes:\n",
    "- PCA identifies directions of maximum *sample* variance, not the true population variance. Strong principal components can appear for independent variables, reflecting sample scale or random co-fluctuations rather than true correlation. \n",
    "- Changing sample size may affect SVD. As the sample size (e.g., days) increases, random effects diminish, and the dominant direction stabilizes if genuine correlations exist.\n",
    "- This is the SVD-based formulation of PCA. Traditional PCA explicitly computes the covariance matrix and then finds its eigenvalues and eigenvectors. The eigenvalues from the SVD (squares of singular values) are the same as from PCA except scaled by a constant factor, typically \n",
    "1/n depending on how covariance is defined. Thus, while the numerical values of the eigenvalues may differ by that scaling constant, the principal component directions are identical.\n",
    "\n",
    "PCA reveals dominant correlation patterns, aiding dimensionality reduction, risk analysis, and noise filtering—since weaker components often represent residual or random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1eb9888a-ed73-41e9-8e40-b0770a8a27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the singular values:\n",
      "   [1.15291338e-01 1.11092306e-01 9.73805149e-02 8.43686795e-02\n",
      " 5.14097015e-17]\n",
      "Here are the associated first 3 principal components in columns of U, in order:\n",
      "  [[ 0.24815575 -0.75162321  0.15124529]\n",
      " [-0.3109321  -0.23204021  0.08968654]\n",
      " [ 0.50923836  0.24648983 -0.68604702]\n",
      " [-0.71453136  0.21246153 -0.22429635]\n",
      " [ 0.26806936  0.52471206  0.66941154]]\n",
      "VARIANCE EXPLAINED: \n",
      "  0.8314635698305499\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "portfolio_returns = np.random.normal(0.05, 0.02, (5, 30))\n",
    "\n",
    "def svd(A, k):\n",
    "    A_centered = A - np.mean(A, axis=0) #Make sure you center before PCA, set the axis depending on whether row = features or col = features.\n",
    "    U, S, VT = np.linalg.svd(A_centered, full_matrices= False) #set full_matrices = false for large matricies \n",
    "    x = np.sum(S[:k]**2) / np.sum(S**2)\n",
    "    print(\"Here are the singular values:\\n  \", S) \n",
    "    print(\"Here are the associated first\", k, \"principal components in columns of U, in order:\\n \", U[:, :k])\n",
    "    print(\"VARIANCE EXPLAINED: \\n \", x)\n",
    "\n",
    "svd(portfolio_returns, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
